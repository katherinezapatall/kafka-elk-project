# Implementing Real-time Logging/Monitoring using Kafka and ELK

## Introduction

In today's technology-driven landscape, ensuring the availability, reliability, and performance of complex microservices-based applications is paramount for business success. To address this need, for this project is designed a centralized logging and monitoring infrastructure that can handle a large amount of logs/messages generated by the microservices.

## Implementation

This project includes the implementation of the infrastructure as code for Real-time Logging/Monitoring using Kafka and ELK. The code consists of 2 main parts:

### Part 1

The first part includes the infrastructure as code to deploy the following microservices:

- Kafka broker 1 and 2: Container 1 and 2 from the diagram "LOGGING/MONITORING ARCHITECTURE"
- Zookeeper: Container 3 from the diagram "LOGGING/MONITORING ARCHITECTURE"
- Logstash: Container 4 from the diagram "LOGGING/MONITORING ARCHITECTURE"
- Elasticsearch: Container 5 from the diagram "LOGGING/MONITORING ARCHITECTURE"
- Kibana: Container 6 from the diagram "LOGGING/MONITORING ARCHITECTURE"

These are the containers required to implement Real-Time Logging and Monitoring, where:

- Kafka: Kafka is used as a distributed message broker to collect log data from all microservices. Kafka's scalability ensures it can efficiently handle the high volume of logs.
- Logstash: Logstash is responsible for data pre-processing and aggregation. It consumes log messages from Kafka, applies transformations, and forwards the data to Elasticsearch.
- Elasticsearch: Elasticsearch serves as the central repository for log data, offering fast indexing and search capabilities, enabling real-time analysis.
- Kibana: Kibana is used as a visualization and monitoring tool, allowing users to create custom dashboards, set up alerts, and explore log data trends.

### Steps to reproduce

1. Install dependencies:
   - Docker
   - Docker Compose
2. Clone the repository
3. Run the following commands inside the folder "infra":
   ```
   docker-compose -f docker-compose-elk.yml up -d
   ```

3. Depending on the machine capacity, it will takes a while until everything is up and running.
4. Verification:
    - Verify that all containers are running: run the command docker ps. The output will be as in the image 2.
        - Get IDs of currently active brokers:
           ```
            docker exec -it zookeeper /bin/zookeeper-shell localhost:2181
            ls /brokers/ids
            ```
        - The output will be:
            ```
            Connecting to localhost:2181

            Welcome to ZooKeeper!

            JLine support is disabled

            WATCHER:

            WatchedEvent state:SyncConnected type:None path:null


            [1, 2]
            ```

### Part 2:

This part involves the development of the application, including the generation of mock logs for producing and consuming messages.

After spinning up the infrastructure, you can test the project by publishing messages to Kafka using the mock application written in Python:

### Steps to reproduce

1. Install dependencies:
    - Python
2. Run the command in the folder mock-data-application
    ```
    python3 log_producer.py
    ```
3. It possible to update the number of mock messages, modifying this range:
    - https://github.com/katherinezapatall/kafka-elk-project/blob/main/mock-data-application/log_producer.py#L30


After sending the messages, we can validate the correct pre-processing and aggregation of our data by Logstash into Elasticsearch by creating an index and dashboard in Kibana. An example could be a dashboard displaying HTTP 400 and 500 errors, as shown in image number 3.

Additionally, a Python application has been created to consume logs from Kafka and write them to the console.


### Steps to reproduce

4. Run the command in the folder mock-data-application.
    ``` 
    python3 log_consumer.py
    ```
The output will be the history of messages from the beginning. Also, I will show the messages consumed in real time. Example:Â 
    ```
    Received message: {"timestamp": 1696924077.1596959, "source": "http_request_processing", "message": "HTTP request processing log message #28", "http_status_code": 500, "remote_ip": "10.x.10.x"}
    Received message: {"timestamp": 1696924077.831316, "source": "authentication", "message": "User authentication id #29 failed", "remote_ip": "10.10.10.10"}
    Received message: {"timestamp": 1696924078.3446631, "source": "authentication", "message": "User authentication id #30 failed", "remote_ip": "10.10.10.10"}
    Received message: {"timestamp": 1696924079.200612, "source": "authentication", "message": "User authentication id #31 failed", "remote_ip": "10.10.10.10"}
    ```