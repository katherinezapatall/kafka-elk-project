# Implementing Real-time Logging/Monitoring using Kafka and ELK

## Introduction

In today's technology-driven landscape, ensuring the availability, reliability, and performance of complex microservices-based applications is paramount for business success. To address this need, for this project is designed a centralized logging and monitoring infrastructure that can handle a large amount of logs/messages generated by the microservices.

## Implementation

This project includes the implementation of the infrastructure as code for Real-time Logging/Monitoring using Kafka and ELK. The code consists of 2 main parts:

### Part 1

The first part includes the infrastructure as code to deploy the following microservices:

-	Kafka broker 1 y 2 : Container 1 y 2 from [Figure 1](images/architecture.png).
-	Zookeeper: Container 3 from [Figure 1](images/architecture.png).
-	Logstash: Container 4 from [Figure 1](images/architecture.png).
-	Elasticsearch: Container 5 from [Figure 1](images/architecture.png).
-	Kibana: Container 6 from [Figure 1](images/architecture.png).
-	Producer application: Container D from [Figure 1](images/architecture.png).
-	Consumer application: Container A- C from [Figure 1](images/architecture.png).


These are the containers required to implement Real-Time Logging and Monitoring, where:

- Kafka: Kafka is used as a distributed message broker to collect log data from all microservices. Kafka's scalability ensures it can efficiently handle the high volume of logs.
- Logstash: Logstash is responsible for data pre-processing and aggregation. It consumes log messages from Kafka, applies transformations, and forwards the data to Elasticsearch.
- Elasticsearch: Elasticsearch serves as the central repository for log data, offering fast indexing and search capabilities, enabling real-time analysis.
- Kibana: Kibana is used as a visualization and monitoring tool, allowing users to create custom dashboards, set up alerts, and explore log data trends.
- Producer application: producing valuable log data concerning their activities and performance.
- Consumer application: Consume the log data from Kafka. 


### Steps to reproduce

1. Install dependencies:
   - Docker
   - Docker Compose
2. Clone the repository
3. Run the following commands inside the folder "infra":
   ```
   docker-compose -f docker-compose-elk.yml up -d
   ```

3. Depending on the machine capacity, it will takes a while until everything is up and running.
4. Verification:
    - Verify that all containers are running: run the command docker ps. The output will be as in the [Figure 2](images/containers.png).
        - Get IDs of currently active brokers:
        
           ```
            docker exec -it zookeeper /bin/zookeeper-shell localhost:2181
            ls /brokers/ids
            ```
        - The output will be:

            ```
            Connecting to localhost:2181

            Welcome to ZooKeeper!

            JLine support is disabled

            WATCHER:

            WatchedEvent state:SyncConnected type:None path:null


            [1, 2]
            ```


### Part 2

This part involves the development of the applications, to produce and consume the messages.
The code can be found in: 

- https://github.com/katherinezapatall/kafka-elk-project/blob/main/mock-data-application

After spinning up the infrastructure, you can validate the messages produced and consumed by checking the logs of “producer” and “consumer” containers, as shown in  [Figure 3](images/producer-logs.png) and [Figure 4](images/consumer-logs.png)

Note that, it is possible to update the number of produced messages, modifying this range: 
- https://github.com/katherinezapatall/kafka-elk-project/blob/main/mock-data-application/log_producer.py#L30

Additionally, you can validate the correct pre-processing and aggregation of the data by Logstash into Elasticsearch by creating an index and dashboard in Kibana. An example could be a dashboard displaying HTTP 400 and 500 errors, as shown in the [Figure 5](images/example-dashboard.png).
 
### Figures List:

- [Figure 1](images/architecture.png). “Real time  Logging/Monitoring architecture”
- [Figure 2](images/containers.png). “Verification of containers running:”
- [Figure 3](images/producer-logs.png). “Producer Logs”
- [Figure 4](images/consumer-logs.png). “Consumer Logs”
- [Figure 5](images/example-dashboard.png). “Kibana Dashboard”
